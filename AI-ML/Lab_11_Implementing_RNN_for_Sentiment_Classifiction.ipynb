{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 11: Implementing and Training a Recurrent Neural Network with Embedding Layer To Do Sentiment Analysis on Movie Reviews Data from IMDB Dataset**\n",
        "\n",
        "**Date:** 15 December 2023\n",
        "\n",
        "**Problem Statement:** Prepare IMDB Movie Reviews data so that it can be fed to the RNN model. In particular do the tranformation into integer vectors and apply the necessary padding. Make use of 10,000 most common words as the input features. Also only consider the first 500 words (amongts the top 10,000 considered) for each review. So each training input example will have exactly 500 features after padding.\n",
        "\n",
        "The RNN model will have an Embedding Layer with 32 dimensional word embeddings. And then a SimpleRNN layer with 32 dimensional output features. Make use of 'rmsprop' as an optimizer and appropriate loss function and accuracy metric for binary classification. Use 'sigmoid' function as the output layer activation function.\n",
        "\n",
        "Train and validate the model. Plot training and validation performance graphically. Reserve 20% of training data for validation purpose.  Finally evaluate the model performance on the Test dataset."
      ],
      "metadata": {
        "id": "Arv9trdY5TZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Prepare the IMDB Movie Reviews Text Data"
      ],
      "metadata": {
        "id": "SdXUYXpv-Zpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDnlaSKG46cD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "311a1f10-a3be-4fa7-9d5f-ad2605c17b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data ...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "input_train shape: (25000, 500)\n",
            "input_test shape: (25000, 500)\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000 # Number of most frequent (or common) words to consider as features\n",
        "maxlen = 500 # Cutts off texts after these many words (among the max_features most common words)\n",
        "batch_size = 128\n",
        "\n",
        "print('Loading data ...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Build, Compile and Train a simple recurrent neural network using an Embedding Layer and a SimpleRNN layer"
      ],
      "metadata": {
        "id": "SoNZNgjh-g1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Embedding, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 32))\n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history = model.fit(input_train, y_train, epochs=10,\n",
        "                    batch_size=batch_size, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BITzTTI--xAq",
        "outputId": "09a7d091-e5e5-45bb-ce5a-86412b8e7c54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 28s 165ms/step - loss: 0.6641 - acc: 0.5878 - val_loss: 0.5408 - val_acc: 0.7260\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 26s 169ms/step - loss: 0.4743 - acc: 0.7804 - val_loss: 0.4701 - val_acc: 0.7808\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 24s 156ms/step - loss: 0.3551 - acc: 0.8525 - val_loss: 0.4218 - val_acc: 0.8126\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 24s 155ms/step - loss: 0.2921 - acc: 0.8828 - val_loss: 0.3720 - val_acc: 0.8392\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 24s 154ms/step - loss: 0.2334 - acc: 0.9104 - val_loss: 0.3673 - val_acc: 0.8568\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.1954 - acc: 0.9276 - val_loss: 0.5103 - val_acc: 0.7786\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 24s 153ms/step - loss: 0.1577 - acc: 0.9424 - val_loss: 0.4454 - val_acc: 0.8270\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 24s 154ms/step - loss: 0.1168 - acc: 0.9592 - val_loss: 0.4687 - val_acc: 0.8524\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 24s 155ms/step - loss: 0.0726 - acc: 0.9770 - val_loss: 0.5755 - val_acc: 0.8286\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 23s 148ms/step - loss: 0.0588 - acc: 0.9827 - val_loss: 0.5914 - val_acc: 0.8112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Plot the Training and Validation set loss and accuracy"
      ],
      "metadata": {
        "id": "zUMJ48RjBMLD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kgI_fuoCBU-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Evaluate the model on test dataset"
      ],
      "metadata": {
        "id": "Dfwya7BXBYhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 (Final Step)**: Use the model to classify the sentiment on a randomly selected movie review from the test dataset"
      ],
      "metadata": {
        "id": "O9OjR0a2Bd8K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJulHO5ZBdde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YjiRugEvBUf-"
      }
    }
  ]
}